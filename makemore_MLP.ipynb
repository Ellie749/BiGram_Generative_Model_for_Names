{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level Language Modeling with MLP\n",
    "architecture by Bengio et. al.\n",
    "\n",
    "\n",
    "\n",
    "2 perspectives to look at Embedding layer: \n",
    "1) direct access through indexing\n",
    "2) One-hot filtering\n",
    "https://chatgpt.com/share/6775af97-1aac-800a-b76a-3a9c2a2043c0\n",
    "\n",
    "\n",
    "\n",
    "This code does not have a concept of sequence that's why we concatenate the embeddings to be multiplied to W - It doesn't care if x comes before y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_to_int(data):\n",
    "    '''\n",
    "    Given a dataset of words(names), char_to_int converts the unique characters to an integer and assigns an id to them.\n",
    "    This is for train step.\n",
    "\n",
    "    Args:\n",
    "        data: a list of names\n",
    "\n",
    "    Returns:\n",
    "        char_ids: a dictionary of keys being characters and values the corrosponding integer id to each token\n",
    "    '''\n",
    "\n",
    "    char_ids = {}\n",
    "    chars = sorted(set(''.join(data)))\n",
    "    char_ids['.'] = 0\n",
    "    for idx, c in enumerate(chars):\n",
    "        char_ids[c] = idx + 1\n",
    "    return char_ids\n",
    "\n",
    "\n",
    "def int_to_char(data: dict):\n",
    "    '''\n",
    "    Given a dataset of ids, int_to_char converts the ids to their original character. This is for inference step.\n",
    "\n",
    "    Args:\n",
    "        data: a dictionary of (chars, ids)\n",
    "\n",
    "    Returns:\n",
    "        char_ids: a dictionary of (ids, chars)\n",
    "    '''\n",
    "    chars = {}\n",
    "    for k, v in data.items():\n",
    "        chars[v] = k\n",
    "\n",
    "    return chars\n",
    "\n",
    "\n",
    "char_ids = char_to_int(words)\n",
    "id_chars = int_to_char(char_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_ids['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(data: list, sequence_length: int) -> torch.tensor:\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for word in data:\n",
    "        context = [0] * sequence_length\n",
    "        for ch in word + '.':\n",
    "            X.append(context)\n",
    "            y.append(char_ids[ch])\n",
    "            c_id = char_ids[ch]\n",
    "            \n",
    "            context = context[1:] + [c_id]\n",
    "            \n",
    "    \n",
    "    return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "\n",
    "X, y = make_dataset(words, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numebr of train samples: 182517\n",
      "Number of validation samples: 22814\n",
      "Number of test samples: 22815\n"
     ]
    }
   ],
   "source": [
    "def data_split(X, y, trian_size):\n",
    "    trian_size = round(X.size()[0]*trian_size)\n",
    "    validation_size = round((X.size()[0] - trian_size)*0.5)\n",
    "\n",
    "    X_train = X[:trian_size]\n",
    "    X_validation = X[trian_size:trian_size+validation_size]\n",
    "    X_test = X[trian_size+validation_size:]\n",
    "\n",
    "    y_train = y[:trian_size]\n",
    "    y_validation = y[trian_size:trian_size+validation_size]\n",
    "    y_test = y[trian_size+validation_size:]\n",
    "\n",
    "    return X_train, X_validation, X_test, y_train, y_validation, y_test\n",
    "\n",
    "\n",
    "X_train, X_validation, X_test, y_train, y_validation, y_test = data_split(X, y, trian_size=0.8)\n",
    "print(f\"Numebr of train samples: {X_train.size()[0]}\")\n",
    "print(f\"Number of validation samples: {X_validation.size()[0]}\")\n",
    "print(f\"Number of test samples: {X_test.size()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 3481\n"
     ]
    }
   ],
   "source": [
    "def create_model(input_shape, layer2_neurons, embedding_dim, n_classes):\n",
    "\n",
    "    C = torch.rand((n_classes, embedding_dim))\n",
    "    # embed = C[input_data] # 27*3*2\n",
    "    # layer1 = embed.view((-1, input_data.size()[-1]*embedding_dim))\n",
    "    W1 = torch.rand((input_shape*embedding_dim, layer2_neurons))\n",
    "    b1  = torch.rand(layer2_neurons)\n",
    "    W2 = torch.rand((layer2_neurons, n_classes))\n",
    "    b2 = torch.rand(n_classes)\n",
    "\n",
    "    return {'C': C, 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "\n",
    "parameters = create_model(X.size()[-1], 100, 2, 27)\n",
    "print(f\"Total number of parameters: {sum(v.nelement() for k, v in parameters.items())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch= 1, iteration=0, loss= 14.770447731018066\n",
      "Epoch= 1, iteration=1, loss= 7.94075870513916\n",
      "Epoch= 1, iteration=2, loss= 8.011354446411133\n",
      "Epoch= 1, iteration=3, loss= 6.2299580574035645\n",
      "Epoch= 1, iteration=4, loss= 5.167121887207031\n",
      "Epoch= 1, iteration=5, loss= 3.960541009902954\n",
      "Epoch= 1, iteration=6, loss= 3.697648048400879\n",
      "Epoch= 1, iteration=7, loss= 3.8183038234710693\n",
      "Epoch= 2, iteration=0, loss= 3.2145400047302246\n",
      "Epoch= 2, iteration=1, loss= 3.07409405708313\n",
      "Epoch= 2, iteration=2, loss= 3.429368019104004\n",
      "Epoch= 2, iteration=3, loss= 2.9411416053771973\n",
      "Epoch= 2, iteration=4, loss= 3.01171612739563\n",
      "Epoch= 2, iteration=5, loss= 2.9506006240844727\n",
      "Epoch= 2, iteration=6, loss= 3.0948920249938965\n",
      "Epoch= 2, iteration=7, loss= 3.135560989379883\n",
      "Epoch= 3, iteration=0, loss= 3.201793670654297\n",
      "Epoch= 3, iteration=1, loss= 2.8715460300445557\n",
      "Epoch= 3, iteration=2, loss= 2.907761812210083\n",
      "Epoch= 3, iteration=3, loss= 3.1215295791625977\n",
      "Epoch= 3, iteration=4, loss= 3.0855050086975098\n",
      "Epoch= 3, iteration=5, loss= 3.497150182723999\n",
      "Epoch= 3, iteration=6, loss= 3.097949981689453\n",
      "Epoch= 3, iteration=7, loss= 3.0643956661224365\n",
      "Epoch= 4, iteration=0, loss= 3.0562338829040527\n",
      "Epoch= 4, iteration=1, loss= 3.026677131652832\n",
      "Epoch= 4, iteration=2, loss= 3.1713123321533203\n",
      "Epoch= 4, iteration=3, loss= 2.8292298316955566\n",
      "Epoch= 4, iteration=4, loss= 2.8421454429626465\n",
      "Epoch= 4, iteration=5, loss= 2.97513484954834\n",
      "Epoch= 4, iteration=6, loss= 3.1718297004699707\n",
      "Epoch= 4, iteration=7, loss= 3.0001959800720215\n",
      "Epoch= 5, iteration=0, loss= 2.859815835952759\n",
      "Epoch= 5, iteration=1, loss= 2.8392274379730225\n",
      "Epoch= 5, iteration=2, loss= 2.8541157245635986\n",
      "Epoch= 5, iteration=3, loss= 2.902606725692749\n",
      "Epoch= 5, iteration=4, loss= 3.036843776702881\n",
      "Epoch= 5, iteration=5, loss= 2.889526128768921\n",
      "Epoch= 5, iteration=6, loss= 2.966087579727173\n",
      "Epoch= 5, iteration=7, loss= 2.9425783157348633\n",
      "Epoch= 6, iteration=0, loss= 2.849821090698242\n",
      "Epoch= 6, iteration=1, loss= 2.7937099933624268\n",
      "Epoch= 6, iteration=2, loss= 2.815009117126465\n",
      "Epoch= 6, iteration=3, loss= 2.805762529373169\n",
      "Epoch= 6, iteration=4, loss= 2.841991662979126\n",
      "Epoch= 6, iteration=5, loss= 2.8605399131774902\n",
      "Epoch= 6, iteration=6, loss= 2.9378550052642822\n",
      "Epoch= 6, iteration=7, loss= 2.891554355621338\n",
      "Epoch= 7, iteration=0, loss= 2.7785396575927734\n",
      "Epoch= 7, iteration=1, loss= 2.7313570976257324\n",
      "Epoch= 7, iteration=2, loss= 2.729576826095581\n",
      "Epoch= 7, iteration=3, loss= 2.740142345428467\n",
      "Epoch= 7, iteration=4, loss= 2.7488036155700684\n",
      "Epoch= 7, iteration=5, loss= 2.811691999435425\n",
      "Epoch= 7, iteration=6, loss= 2.892285108566284\n",
      "Epoch= 7, iteration=7, loss= 2.87725830078125\n",
      "Epoch= 8, iteration=0, loss= 2.7623205184936523\n",
      "Epoch= 8, iteration=1, loss= 2.7194559574127197\n",
      "Epoch= 8, iteration=2, loss= 2.7157392501831055\n",
      "Epoch= 8, iteration=3, loss= 2.727191686630249\n",
      "Epoch= 8, iteration=4, loss= 2.7335896492004395\n",
      "Epoch= 8, iteration=5, loss= 2.7960591316223145\n",
      "Epoch= 8, iteration=6, loss= 2.879851818084717\n",
      "Epoch= 8, iteration=7, loss= 2.872061252593994\n",
      "Epoch= 9, iteration=0, loss= 2.7542765140533447\n",
      "Epoch= 9, iteration=1, loss= 2.7127456665039062\n",
      "Epoch= 9, iteration=2, loss= 2.70902681350708\n",
      "Epoch= 9, iteration=3, loss= 2.7200708389282227\n",
      "Epoch= 9, iteration=4, loss= 2.7265396118164062\n",
      "Epoch= 9, iteration=5, loss= 2.7879486083984375\n",
      "Epoch= 9, iteration=6, loss= 2.8732645511627197\n",
      "Epoch= 9, iteration=7, loss= 2.86747407913208\n",
      "Epoch= 10, iteration=0, loss= 2.7479894161224365\n",
      "Epoch= 10, iteration=1, loss= 2.707247734069824\n",
      "Epoch= 10, iteration=2, loss= 2.7034900188446045\n",
      "Epoch= 10, iteration=3, loss= 2.7140488624572754\n",
      "Epoch= 10, iteration=4, loss= 2.720564603805542\n",
      "Epoch= 10, iteration=5, loss= 2.780916452407837\n",
      "Epoch= 10, iteration=6, loss= 2.867616653442383\n",
      "Epoch= 10, iteration=7, loss= 2.863196611404419\n"
     ]
    }
   ],
   "source": [
    "def train(parameters, X_train, y_train, embedding_dim, batch_size=0, epochs=10, lr=0.1):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(batch_size):\n",
    "            data = X_train[batch*(X_train.size()[0] // batch_size): (X_train.size()[0] // batch_size)*(batch+1)]\n",
    "            label = y_train[batch*(y_train.size()[0] // batch_size): (y_train.size()[0] // batch_size)*(batch+1)]\n",
    "            embeds = parameters['C'][data]\n",
    "            layer1 = embeds.view((-1, data.size()[-1]*embedding_dim))\n",
    "            h = torch.tanh(layer1 @ parameters['W1'])*parameters['b1']\n",
    "            logits = (h @ parameters['W2'])*parameters['b2']\n",
    "            loss = F.cross_entropy(logits, label)\n",
    "            print(f\"Epoch= {epoch+1}, iteration={batch}, loss= {loss}\")\n",
    "            \n",
    "            for _, v in parameters.items():\n",
    "                v.grad = None\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            for _, v in parameters.items():\n",
    "                v.data += -lr*v.grad\n",
    "        \n",
    "for k,v in parameters.items():\n",
    "    v.requires_grad = True\n",
    "\n",
    "train(parameters, X_train, y_train, 2, batch_size=8, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax + log likelihood = CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merging dimensions is always from the most inner dimension\n",
    "\n",
    "cross entropy comapres only with relative to the correct label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crossentropy is preferred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python PyTorch_AndrejKarpathy_venv",
   "language": "python",
   "name": "pytorch_andrejkarpathy_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
