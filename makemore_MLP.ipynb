{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level Language Modeling with MLP\n",
    "architecture by Bengio et. al.\n",
    "\n",
    "\n",
    "\n",
    "2 perspectives to look at Embedding layer: \n",
    "1) direct access through indexing\n",
    "2) One-hot filtering\n",
    "https://chatgpt.com/share/6775af97-1aac-800a-b76a-3a9c2a2043c0\n",
    "\n",
    "\n",
    "\n",
    "This code does not have a concept of sequence that's why we concatenate the embeddings to be multiplied to W - It doesn't care if x comes before y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_to_int(data):\n",
    "    '''\n",
    "    Given a dataset of words(names), char_to_int converts the unique characters to an integer and assigns an id to them.\n",
    "    This is for train step.\n",
    "\n",
    "    Args:\n",
    "        data: a list of names\n",
    "\n",
    "    Returns:\n",
    "        char_ids: a dictionary of keys being characters and values the corrosponding integer id to each token\n",
    "    '''\n",
    "\n",
    "    char_ids = {}\n",
    "    chars = sorted(set(''.join(data)))\n",
    "    char_ids['.'] = 0\n",
    "    for idx, c in enumerate(chars):\n",
    "        char_ids[c] = idx + 1\n",
    "    return char_ids\n",
    "\n",
    "\n",
    "def int_to_char(data: dict):\n",
    "    '''\n",
    "    Given a dataset of ids, int_to_char converts the ids to their original character. This is for inference step.\n",
    "\n",
    "    Args:\n",
    "        data: a dictionary of (chars, ids)\n",
    "\n",
    "    Returns:\n",
    "        char_ids: a dictionary of (ids, chars)\n",
    "    '''\n",
    "    chars = {}\n",
    "    for k, v in data.items():\n",
    "        chars[v] = k\n",
    "\n",
    "    return chars\n",
    "\n",
    "\n",
    "char_ids = char_to_int(words)\n",
    "id_chars = int_to_char(char_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(data: list, sequence_length: int) -> torch.tensor:\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for word in data:\n",
    "        context = [0] * sequence_length\n",
    "        for ch in word + '.':\n",
    "            X.append(context)\n",
    "            y.append(char_ids[ch])\n",
    "            c_id = char_ids[ch]\n",
    "            \n",
    "            context = context[1:] + [c_id]\n",
    "            \n",
    "    \n",
    "    return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "\n",
    "X, y = make_dataset(words, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numebr of train samples: 182517\n",
      "Number of validation samples: 22814\n",
      "Number of test samples: 22815\n"
     ]
    }
   ],
   "source": [
    "def data_split(X, y, trian_size):\n",
    "    trian_size = round(X.size()[0]*trian_size)\n",
    "    validation_size = round((X.size()[0] - trian_size)*0.5)\n",
    "\n",
    "    X_train = X[:trian_size]\n",
    "    X_validation = X[trian_size:trian_size+validation_size]\n",
    "    X_test = X[trian_size+validation_size:]\n",
    "\n",
    "    y_train = y[:trian_size]\n",
    "    y_validation = y[trian_size:trian_size+validation_size]\n",
    "    y_test = y[trian_size+validation_size:]\n",
    "\n",
    "    return X_train, X_validation, X_test, y_train, y_validation, y_test\n",
    "\n",
    "\n",
    "X_train, X_validation, X_test, y_train, y_validation, y_test = data_split(X, y, trian_size=0.8)\n",
    "print(f\"Numebr of train samples: {X_train.size()[0]}\")\n",
    "print(f\"Number of validation samples: {X_validation.size()[0]}\")\n",
    "print(f\"Number of test samples: {X_test.size()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 3481\n"
     ]
    }
   ],
   "source": [
    "def create_model(input_shape, layer2_neurons, embedding_dim, n_classes):\n",
    "\n",
    "    C = torch.rand((n_classes, embedding_dim))\n",
    "    # embed = C[input_data] # 27*3*2\n",
    "    # layer1 = embed.view((-1, input_data.size()[-1]*embedding_dim))\n",
    "    W1 = torch.rand((input_shape*embedding_dim, layer2_neurons))\n",
    "    b1  = torch.rand(layer2_neurons)\n",
    "    W2 = torch.rand((layer2_neurons, n_classes))\n",
    "    b2 = torch.rand(n_classes)\n",
    "\n",
    "    return {'C': C, 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "\n",
    "parameters = create_model(X.size()[-1], 100, 2, 27)\n",
    "print(f\"Total number of parameters: {sum(v.nelement() for k, v in parameters.items())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch= 1, iteration=0, loss= 10.953051567077637\n",
      "Epoch= 1, iteration=1, loss= 7.868553638458252\n",
      "Epoch= 1, iteration=2, loss= 6.843010425567627\n",
      "Epoch= 1, iteration=3, loss= 5.887442111968994\n",
      "Epoch= 1, iteration=4, loss= 7.735470771789551\n",
      "Epoch= 1, iteration=5, loss= 8.262130737304688\n",
      "Epoch= 1, iteration=6, loss= 5.98015832901001\n",
      "Epoch= 1, iteration=7, loss= 4.598536491394043\n",
      "Epoch= 2, iteration=0, loss= 3.715553045272827\n",
      "Epoch= 2, iteration=1, loss= 3.0508549213409424\n",
      "Epoch= 2, iteration=2, loss= 3.076706647872925\n",
      "Epoch= 2, iteration=3, loss= 3.1377391815185547\n",
      "Epoch= 2, iteration=4, loss= 2.9129652976989746\n",
      "Epoch= 2, iteration=5, loss= 2.9304914474487305\n",
      "Epoch= 2, iteration=6, loss= 3.075181007385254\n",
      "Epoch= 2, iteration=7, loss= 3.0618844032287598\n",
      "Epoch= 3, iteration=0, loss= 2.968907117843628\n",
      "Epoch= 3, iteration=1, loss= 2.7882468700408936\n",
      "Epoch= 3, iteration=2, loss= 2.7739949226379395\n",
      "Epoch= 3, iteration=3, loss= 2.778329849243164\n",
      "Epoch= 3, iteration=4, loss= 2.809187412261963\n",
      "Epoch= 3, iteration=5, loss= 2.8672118186950684\n",
      "Epoch= 3, iteration=6, loss= 2.9973368644714355\n",
      "Epoch= 3, iteration=7, loss= 2.97542667388916\n",
      "Epoch= 4, iteration=0, loss= 2.8810129165649414\n",
      "Epoch= 4, iteration=1, loss= 2.7580807209014893\n",
      "Epoch= 4, iteration=2, loss= 2.758589267730713\n",
      "Epoch= 4, iteration=3, loss= 2.7627995014190674\n",
      "Epoch= 4, iteration=4, loss= 2.7940006256103516\n",
      "Epoch= 4, iteration=5, loss= 2.83313250541687\n",
      "Epoch= 4, iteration=6, loss= 2.9676034450531006\n",
      "Epoch= 4, iteration=7, loss= 2.9546313285827637\n",
      "Epoch= 5, iteration=0, loss= 2.8575267791748047\n",
      "Epoch= 5, iteration=1, loss= 2.750394105911255\n",
      "Epoch= 5, iteration=2, loss= 2.7520580291748047\n",
      "Epoch= 5, iteration=3, loss= 2.7546706199645996\n",
      "Epoch= 5, iteration=4, loss= 2.7811179161071777\n",
      "Epoch= 5, iteration=5, loss= 2.82026743888855\n",
      "Epoch= 5, iteration=6, loss= 2.9497172832489014\n",
      "Epoch= 5, iteration=7, loss= 2.9408130645751953\n",
      "Epoch= 6, iteration=0, loss= 2.8346898555755615\n",
      "Epoch= 6, iteration=1, loss= 2.744009256362915\n",
      "Epoch= 6, iteration=2, loss= 2.745621681213379\n",
      "Epoch= 6, iteration=3, loss= 2.747218608856201\n",
      "Epoch= 6, iteration=4, loss= 2.7695744037628174\n",
      "Epoch= 6, iteration=5, loss= 2.810129404067993\n",
      "Epoch= 6, iteration=6, loss= 2.9349348545074463\n",
      "Epoch= 6, iteration=7, loss= 2.928732395172119\n",
      "Epoch= 7, iteration=0, loss= 2.81500506401062\n",
      "Epoch= 7, iteration=1, loss= 2.738356351852417\n",
      "Epoch= 7, iteration=2, loss= 2.7395172119140625\n",
      "Epoch= 7, iteration=3, loss= 2.7407641410827637\n",
      "Epoch= 7, iteration=4, loss= 2.7599117755889893\n",
      "Epoch= 7, iteration=5, loss= 2.801823377609253\n",
      "Epoch= 7, iteration=6, loss= 2.923027276992798\n",
      "Epoch= 7, iteration=7, loss= 2.918558359146118\n",
      "Epoch= 8, iteration=0, loss= 2.7988500595092773\n",
      "Epoch= 8, iteration=1, loss= 2.7333264350891113\n",
      "Epoch= 8, iteration=2, loss= 2.7337610721588135\n",
      "Epoch= 8, iteration=3, loss= 2.735198497772217\n",
      "Epoch= 8, iteration=4, loss= 2.7518832683563232\n",
      "Epoch= 8, iteration=5, loss= 2.795036554336548\n",
      "Epoch= 8, iteration=6, loss= 2.913466215133667\n",
      "Epoch= 8, iteration=7, loss= 2.910118579864502\n",
      "Epoch= 9, iteration=0, loss= 2.785802125930786\n",
      "Epoch= 9, iteration=1, loss= 2.7288522720336914\n",
      "Epoch= 9, iteration=2, loss= 2.7284209728240967\n",
      "Epoch= 9, iteration=3, loss= 2.7303519248962402\n",
      "Epoch= 9, iteration=4, loss= 2.745138168334961\n",
      "Epoch= 9, iteration=5, loss= 2.7894177436828613\n",
      "Epoch= 9, iteration=6, loss= 2.9056344032287598\n",
      "Epoch= 9, iteration=7, loss= 2.9030354022979736\n",
      "Epoch= 10, iteration=0, loss= 2.7752063274383545\n",
      "Epoch= 10, iteration=1, loss= 2.7248222827911377\n",
      "Epoch= 10, iteration=2, loss= 2.723491907119751\n",
      "Epoch= 10, iteration=3, loss= 2.7260355949401855\n",
      "Epoch= 10, iteration=4, loss= 2.7393364906311035\n",
      "Epoch= 10, iteration=5, loss= 2.7846384048461914\n",
      "Epoch= 10, iteration=6, loss= 2.8990111351013184\n",
      "Epoch= 10, iteration=7, loss= 2.8969428539276123\n"
     ]
    }
   ],
   "source": [
    "def train(parameters, X_train, y_train, embedding_dim, batch_size=0, epochs=10, lr=0.1):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(batch_size):\n",
    "            data = X_train[batch*(X_train.size()[0] // batch_size): (X_train.size()[0] // batch_size)*(batch+1)]\n",
    "            label = y_train[batch*(y_train.size()[0] // batch_size): (y_train.size()[0] // batch_size)*(batch+1)]\n",
    "            embeds = parameters['C'][data]\n",
    "            layer1 = embeds.view((-1, data.size()[-1]*embedding_dim))\n",
    "            h = torch.tanh(layer1 @ parameters['W1'])*parameters['b1']\n",
    "            logits = (h @ parameters['W2'])*parameters['b2']\n",
    "            loss = F.cross_entropy(logits, label)\n",
    "            print(f\"Epoch= {epoch+1}, iteration={batch}, loss= {loss}\")\n",
    "            \n",
    "            for _, v in parameters.items():\n",
    "                v.grad = None\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            for _, v in parameters.items():\n",
    "                v.data += -lr*v.grad\n",
    "\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n",
    "        \n",
    "for k,v in parameters.items():\n",
    "    v.requires_grad = True\n",
    "\n",
    "model = train(parameters, X_train, y_train, 2, batch_size=8, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax + log likelihood = CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 14, 14])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['iijrashermaaatau', 'hzanelri', 'srnreydl', 'sarcnaie', '', 'noayilaodarnv', 'damnyaiairdmnidondhyshrra', 'erlnoayyinnydoleldiiqioahonsvraimyyalagery', 'rdaiiz', 'cesnateiueiseebaeysnen']\n"
     ]
    }
   ],
   "source": [
    "def inference(parameters, n_names):\n",
    "\n",
    "    sequence_length = parameters['W1'].size()[0] // parameters['C'].size()[1]\n",
    "    tokens = [0] * sequence_length\n",
    "\n",
    "    names = []\n",
    "\n",
    "    for _ in range(n_names):\n",
    "        chars = []\n",
    "        while True:\n",
    "            embed = parameters['C'][tokens]\n",
    "            layer1 = embed.view((-1, sequence_length*parameters['C'].size()[1]))\n",
    "            h = torch.tanh(layer1 @ parameters['W1'])*parameters['b1']\n",
    "            logits = (h @ parameters['W2'])*parameters['b2']\n",
    "            probs = torch.softmax(logits.data[0], dim=0)\n",
    "            pred = torch.multinomial(probs, 1, replacement=True)\n",
    "            if pred==0:\n",
    "                break\n",
    "            chars.append(id_chars[pred.item()])\n",
    "\n",
    "            tokens = tokens[1:] + [pred.item()]\n",
    "\n",
    "        names.append(chars)\n",
    "\n",
    "    return names\n",
    "    \n",
    "print([''.join(i) for i in inference(model, 10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merging dimensions is always from the most inner dimension\n",
    "\n",
    "cross entropy comapres only with relative to the correct label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crossentropy is preferred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python PyTorch_AndrejKarpathy_venv",
   "language": "python",
   "name": "pytorch_andrejkarpathy_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
