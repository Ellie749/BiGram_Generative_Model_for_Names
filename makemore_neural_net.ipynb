{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_to_int(data):\n",
    "    char_ids = {}\n",
    "    chars = sorted(set(''.join(data)))\n",
    "\n",
    "    for idx, c in enumerate(chars):\n",
    "        char_ids[c] = idx + 1\n",
    "    \n",
    "    char_ids['.'] = 0\n",
    "\n",
    "    return char_ids\n",
    "\n",
    "\n",
    "def int_to_char(data):\n",
    "    int_char = {v:k for k,v in data.items()}\n",
    "    return int_char\n",
    "\n",
    "\n",
    "char_ids = char_to_int(words)\n",
    "id_char = int_to_char(char_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataest(data, ids):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for w in data:\n",
    "        s = ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2 in zip(s, s[1:]):\n",
    "            X.append(ids[ch1])\n",
    "            y.append(ids[ch2])\n",
    "    \n",
    "    return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "data, label = make_dataest(words[:6], char_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 3.8021974563598633\n",
      "Epoch: 1, loss: 3.4321272373199463\n",
      "Epoch: 2, loss: 3.111724615097046\n",
      "Epoch: 3, loss: 2.8578903675079346\n",
      "Epoch: 4, loss: 2.661515474319458\n",
      "Epoch: 5, loss: 2.498030185699463\n",
      "Epoch: 6, loss: 2.355353593826294\n",
      "Epoch: 7, loss: 2.229495048522949\n",
      "Epoch: 8, loss: 2.1183438301086426\n",
      "Epoch: 9, loss: 2.020134210586548\n"
     ]
    }
   ],
   "source": [
    "def train(X, y, epochs):\n",
    "    W = torch.randn((27,27), requires_grad=True)\n",
    "    for epoch in range(epochs):\n",
    "        xenc = F.one_hot(X, 27).float()   # one hot function only accepts integer values\n",
    "        logits = xenc @ W  # W acts as the same matrix (P) in probabilistic method and xenc acts as w[idx] which triggers the right row\n",
    "        e = torch.exp(logits)\n",
    "        probs = e / e.sum(dim=1, keepdim=True)\n",
    "        loss = -probs[range(len(X)), y].log().mean() + 0.1*(W**2).mean() # regularization in here is like adding 1 to N, (N+1)\n",
    "        print(f\"Epoch: {epoch}, loss: {loss}\")\n",
    "\n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "        W.data += -10 * W.grad\n",
    "\n",
    "    return W\n",
    "\n",
    "model = train(data, label, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['snxdeooqhnmophyvjcxjxttehisa', 'xeeeloxd', 'kyma', 'e', 'sambuta', 'a', 'mia', 'ymqvnc', 'e', 'egjmfoxbel']\n"
     ]
    }
   ],
   "source": [
    "def inference(model, num_words, id_char):\n",
    "    names = []\n",
    "    idx = 0\n",
    "    for i in range(num_words):\n",
    "        name = ''\n",
    "        while True:\n",
    "            x_enc = F.one_hot(torch.tensor([idx]), num_classes=27).float()\n",
    "            logits = x_enc @ model\n",
    "            p = logits.exp()\n",
    "            p = p / p.sum(dim=1, keepdims=True)\n",
    "            idx = torch.multinomial(p, num_samples=1, replacement=True).item() # it should be probability value\n",
    "            # Model in keras or Torch do not have Multinomial. They only calculate until logits or softmax and then we always took argmax\n",
    "            # However instead of argmax, for variery, we can take randomly based on their probabilities using multinomial function\n",
    "            if idx == 0:\n",
    "                break\n",
    "            name += name.join(id_char[idx])\n",
    "\n",
    "        names.append(name)\n",
    "\n",
    "    return names\n",
    "\n",
    "\n",
    "names = inference(model, 10, id_char)\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label smoothing in neural net is to try making the Ws close to each other which means giving the same weight to each data\n",
    "\n",
    "\n",
    "\n",
    "diffusion models learn the distribution space of each pixel probability based on their neighborhood with other pixels (instead of sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python PyTorch_AndrejKarpathy_venv",
   "language": "python",
   "name": "pytorch_andrejkarpathy_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
